---
aliases: [GLUE, General Language Understanding Evaluation]
---

The __General Language Understanding Evaluation (GLUE)__ is a benchmark used to train, evaluate and analyze [[NLP]] systems capable of language understanding.

It has many datasets with different genres, sizes and difficulties.

It is used with a leaderboard, so one compares the performance of their model next to other well known ones.