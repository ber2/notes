This is a blog post in Eva Mart√≠nez's personal blog about a basic description of what is AB testing, how to do it from a Bayesian point of view and approaches to solving the [[Explore vs Exploit Dilemma]].

- blog [post](https://evadatinez.github.io/2020/05/23/ab_test_explore_exploit/)

The whole post is based around an example with [[CTR]].

## Traditional AB Testing

After creating a business case comparing the resources needed to implement the test and the expected/desired gain, the traditional way to handle an A/B test is:
- Select a sample
- Split traffic (for example 50% to each option)
- Run the test until enough data has been collected.
- Perform a hypothesis test on the results.

For clicks on impressions, data follows a [[Bernoulli distribution]] and the obvious statistical test is the [[Chi-square test]], typically with a $p < 0.05$.

The obvious pit for proceeding this way is that one cannot optimize resources until after the experiment has concluded.

## Explore vs Exploit

The [[Explore vs Exploit Dilemma]] is the choice between continuation of the experiment in order to allow new unbiased data to be collected, and exploiting the results already available.

Three strategies are briefly discussed:
- [[Epsilon-greedy]]
- [[UCB]]
- [[Thompson Sampling]]