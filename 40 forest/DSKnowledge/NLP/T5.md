The __T5__ model is a multitask [[NLP]] model that uses a similar strategy to [[BERT]]. 

It uses [[Transfer Learning]]. First, it pre-trains on mask language modeling tasks first and then fine-tunes to the task at hand. It also has an architecture based on [[Transformer]]s.

## Model architecture

- Uses an encoder / decoder stack
- 12 transformer blocks each
- 220M parameters

## Multi-task training strategy

We append a tag prefix to the sentence fed into the model to indicate the task we are training for. Eg:

```
translate English to German: That is good --> Das ist Gut
```

## Data training strategies

- Examples proportional mixing: equal proportion of samples taken for the training dataset (regardless of how much data you have for each task).
- Equal mixing. Take a fixed amount of samples from each task.
- Temperature-scaled mixing: a mixture of both approaches.

## Gradual-unfreezing

Given the layers for the trained model, you fix all of them and start un-freezing them sorted by closest to the output, one at a time, while fine-tuning.

## Adapter layers

You add new [[Neural Network]] layers to each feed-forward of the transformer. The parameters in the new layer fine tune to the specific task.