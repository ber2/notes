## Week 2

The whole week revolves around the central topic of [[Latent Variable]] models and how to train them through [[Expectation Maximization]].

### Latent Variable models

The concept of [[Latent Variable]] is introduced and discussed. Next, it is put in practice by discussing a [[GMM]] for a [[Probabilistic Clustering]] problem.

In order to train this [[GMM]], a toy example of [[Expectation Maximization]] is introduced and discussed, before moving on cover the topic in general.

### Expectation Maximization algorithm

The [[Expectation Maximization]] algorithm is discussed in detail. [[Concave function]]s are discussed, together with [[Jensen's inequality]] and the [[Kullback-Leibler divergence]], which are used to prove the mathematical details in the algorithm: the [[Expectation Maximization#E-step]] and the [[Expectation Maximization#M-step]].